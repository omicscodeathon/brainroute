{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05789458",
   "metadata": {},
   "source": [
    "## Generating clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "591acabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original_Name</th>\n",
       "      <th>Original_SMILES</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>ALogP</th>\n",
       "      <th>ALogp2</th>\n",
       "      <th>AMR</th>\n",
       "      <th>apol</th>\n",
       "      <th>naAromAtom</th>\n",
       "      <th>nAromBond</th>\n",
       "      <th>nAtom</th>\n",
       "      <th>...</th>\n",
       "      <th>P1s</th>\n",
       "      <th>P2s</th>\n",
       "      <th>E1s</th>\n",
       "      <th>E2s</th>\n",
       "      <th>E3s</th>\n",
       "      <th>Ts</th>\n",
       "      <th>As</th>\n",
       "      <th>Vs</th>\n",
       "      <th>Ks</th>\n",
       "      <th>Ds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sulphasalazine</td>\n",
       "      <td>O=C(O)c1cc(N=Nc2ccc(S(=O)(=O)Nc3ccccn3)cc2)ccc1O</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.6366</td>\n",
       "      <td>2.678460</td>\n",
       "      <td>20.9255</td>\n",
       "      <td>52.325102</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>moxalactam</td>\n",
       "      <td>COC1(NC(=O)C(C(=O)O)c2ccc(O)cc2)C(=O)N2C(C(=O)...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.8532</td>\n",
       "      <td>3.434350</td>\n",
       "      <td>84.1596</td>\n",
       "      <td>65.253860</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726012</td>\n",
       "      <td>0.202821</td>\n",
       "      <td>0.515082</td>\n",
       "      <td>0.311531</td>\n",
       "      <td>0.404004</td>\n",
       "      <td>22.493883</td>\n",
       "      <td>107.951242</td>\n",
       "      <td>249.714723</td>\n",
       "      <td>0.589017</td>\n",
       "      <td>1.230617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clioquinol</td>\n",
       "      <td>Oc1c(I)cc(Cl)c2cccnc12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7041</td>\n",
       "      <td>2.903957</td>\n",
       "      <td>22.1264</td>\n",
       "      <td>28.605965</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636046</td>\n",
       "      <td>0.348308</td>\n",
       "      <td>0.488440</td>\n",
       "      <td>0.490672</td>\n",
       "      <td>0.207540</td>\n",
       "      <td>7.749546</td>\n",
       "      <td>14.229627</td>\n",
       "      <td>23.592414</td>\n",
       "      <td>0.476530</td>\n",
       "      <td>1.186652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bbcpd11 (cimetidine analog) (y-g13)</td>\n",
       "      <td>CCNC(=NCCSCc1ncccc1Br)NC#N</td>\n",
       "      <td>0</td>\n",
       "      <td>1.3081</td>\n",
       "      <td>1.711126</td>\n",
       "      <td>58.1882</td>\n",
       "      <td>43.238688</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>schembl614298</td>\n",
       "      <td>CN1CC[C@]23c4c5ccc(OC6O[C@H](C(=O)O)[C@@H](O)[...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.3618</td>\n",
       "      <td>5.578099</td>\n",
       "      <td>88.3588</td>\n",
       "      <td>66.801411</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1877 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Original_Name  \\\n",
       "0                       sulphasalazine   \n",
       "1                           moxalactam   \n",
       "2                           clioquinol   \n",
       "3  bbcpd11 (cimetidine analog) (y-g13)   \n",
       "4                        schembl614298   \n",
       "\n",
       "                                     Original_SMILES  nAcid   ALogP    ALogp2  \\\n",
       "0   O=C(O)c1cc(N=Nc2ccc(S(=O)(=O)Nc3ccccn3)cc2)ccc1O      1 -1.6366  2.678460   \n",
       "1  COC1(NC(=O)C(C(=O)O)c2ccc(O)cc2)C(=O)N2C(C(=O)...      4 -1.8532  3.434350   \n",
       "2                             Oc1c(I)cc(Cl)c2cccnc12      0  1.7041  2.903957   \n",
       "3                         CCNC(=NCCSCc1ncccc1Br)NC#N      0  1.3081  1.711126   \n",
       "4  CN1CC[C@]23c4c5ccc(OC6O[C@H](C(=O)O)[C@@H](O)[...      1 -2.3618  5.578099   \n",
       "\n",
       "       AMR       apol  naAromAtom  nAromBond  nAtom  ...       P1s       P2s  \\\n",
       "0  20.9255  52.325102          18         18     42  ...       NaN       NaN   \n",
       "1  84.1596  65.253860          11         11     56  ...  0.726012  0.202821   \n",
       "2  22.1264  28.605965          10         11     18  ...  0.636046  0.348308   \n",
       "3  58.1882  43.238688           6          6     35  ...       NaN       NaN   \n",
       "4  88.3588  66.801411           6          6     60  ...       NaN       NaN   \n",
       "\n",
       "        E1s       E2s       E3s         Ts          As          Vs        Ks  \\\n",
       "0       NaN       NaN       NaN        NaN         NaN         NaN       NaN   \n",
       "1  0.515082  0.311531  0.404004  22.493883  107.951242  249.714723  0.589017   \n",
       "2  0.488440  0.490672  0.207540   7.749546   14.229627   23.592414  0.476530   \n",
       "3       NaN       NaN       NaN        NaN         NaN         NaN       NaN   \n",
       "4       NaN       NaN       NaN        NaN         NaN         NaN       NaN   \n",
       "\n",
       "         Ds  \n",
       "0       NaN  \n",
       "1  1.230617  \n",
       "2  1.186652  \n",
       "3       NaN  \n",
       "4       NaN  \n",
       "\n",
       "[5 rows x 1877 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/padel_results_loop.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12f523a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9584, 1877)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8192149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Original_Name  \\\n",
      "0                          sulphasalazine   \n",
      "3     bbcpd11 (cimetidine analog) (y-g13)   \n",
      "4                           schembl614298   \n",
      "6                  morphine-6-glucuronide   \n",
      "7                          nitrofurantoin   \n",
      "...                                   ...   \n",
      "9576                            zimeldine   \n",
      "9580    ademetionine(adenosyl-methionine)   \n",
      "9581                             mesocarb   \n",
      "9582                           tofisoline   \n",
      "9583                        azidamfenicol   \n",
      "\n",
      "                                        Original_SMILES  nAcid   ALogP  \\\n",
      "0      O=C(O)c1cc(N=Nc2ccc(S(=O)(=O)Nc3ccccn3)cc2)ccc1O      1 -1.6366   \n",
      "3                            CCNC(=NCCSCc1ncccc1Br)NC#N      0  1.3081   \n",
      "4     CN1CC[C@]23c4c5ccc(OC6O[C@H](C(=O)O)[C@@H](O)[...      1 -2.3618   \n",
      "6     CN1CC[C@]23c4c5ccc(O)c4O[C@H]2[C@@H](OC2O[C@H]...      1 -2.3618   \n",
      "7             O=C1CN(/N=C/c2ccc([N+](=O)[O-])o2)C(=O)N1      0 -0.4812   \n",
      "...                                                 ...    ...     ...   \n",
      "9576          C2=C(\\C(C1=CC=CN=C1)=C\\CN(C)C)C=CC(=C2)Br      0  1.7918   \n",
      "9580  [C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](...      0 -4.7530   \n",
      "9581  [O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=...      0  1.4656   \n",
      "9582  C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC...      0 -0.8870   \n",
      "9583  [N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]...      0 -2.8918   \n",
      "\n",
      "         ALogp2       AMR       apol  naAromAtom  nAromBond  nAtom  ...  P1s  \\\n",
      "0      2.678460   20.9255  52.325102          18         18     42  ...  NaN   \n",
      "3      1.711126   58.1882  43.238688           6          6     35  ...  NaN   \n",
      "4      5.578099   88.3588  66.801411           6          6     60  ...  NaN   \n",
      "6      5.578099   88.3588  66.801411           6          6     60  ...  NaN   \n",
      "7      0.231553   32.5062  26.490758           5          5     23  ...  NaN   \n",
      "...         ...       ...        ...         ...        ...    ...  ...  ...   \n",
      "9576   3.210547   91.6133  44.745481           0          0     36  ...  NaN   \n",
      "9580  22.591009   84.4263  54.579446           0          0     49  ...  NaN   \n",
      "9581   2.147983   98.9391  49.686274           0          0     42  ...  NaN   \n",
      "9582   0.786769  113.2376  61.464618           0          0     54  ...  NaN   \n",
      "9583   8.362507   66.5941  37.538309           0          0     34  ...  NaN   \n",
      "\n",
      "      P2s  E1s  E2s  E3s  Ts  As  Vs  Ks  Ds  \n",
      "0     NaN  NaN  NaN  NaN NaN NaN NaN NaN NaN  \n",
      "3     NaN  NaN  NaN  NaN NaN NaN NaN NaN NaN  \n",
      "4     NaN  NaN  NaN  NaN NaN NaN NaN NaN NaN  \n",
      "6     NaN  NaN  NaN  NaN NaN NaN NaN NaN NaN  \n",
      "7     NaN  NaN  NaN  NaN NaN NaN NaN NaN NaN  \n",
      "...   ...  ...  ...  ...  ..  ..  ..  ..  ..  \n",
      "9576  NaN  NaN  NaN  NaN NaN NaN NaN NaN NaN  \n",
      "9580  NaN  NaN  NaN  NaN NaN NaN NaN NaN NaN  \n",
      "9581  NaN  NaN  NaN  NaN NaN NaN NaN NaN NaN  \n",
      "9582  NaN  NaN  NaN  NaN NaN NaN NaN NaN NaN  \n",
      "9583  NaN  NaN  NaN  NaN NaN NaN NaN NaN NaN  \n",
      "\n",
      "[3272 rows x 1877 columns]\n"
     ]
    }
   ],
   "source": [
    "rows_missing_df = df[df.isnull().any(axis=1)]\n",
    "print(rows_missing_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54094bfc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a772fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing values: 3272\n"
     ]
    }
   ],
   "source": [
    "rows_with_missing = df.isna().any(axis=1).sum()\n",
    "print(\"Rows with missing values:\", rows_with_missing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59fbb6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6312, 1877)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = df.dropna(axis=0,)    \n",
    "clean_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a1b57b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wdjmxmbx5lz8mb2nqmnbfvqm0000gp/T/ipykernel_73075/1140481586.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df.rename(columns={'Original_SMILES':'smiles'}, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original_Name</th>\n",
       "      <th>smiles</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>ALogP</th>\n",
       "      <th>ALogp2</th>\n",
       "      <th>AMR</th>\n",
       "      <th>apol</th>\n",
       "      <th>naAromAtom</th>\n",
       "      <th>nAromBond</th>\n",
       "      <th>nAtom</th>\n",
       "      <th>...</th>\n",
       "      <th>P1s</th>\n",
       "      <th>P2s</th>\n",
       "      <th>E1s</th>\n",
       "      <th>E2s</th>\n",
       "      <th>E3s</th>\n",
       "      <th>Ts</th>\n",
       "      <th>As</th>\n",
       "      <th>Vs</th>\n",
       "      <th>Ks</th>\n",
       "      <th>Ds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>moxalactam</td>\n",
       "      <td>COC1(NC(=O)C(C(=O)O)c2ccc(O)cc2)C(=O)N2C(C(=O)...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.8532</td>\n",
       "      <td>3.434350</td>\n",
       "      <td>84.1596</td>\n",
       "      <td>65.253860</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.726012</td>\n",
       "      <td>0.202821</td>\n",
       "      <td>0.515082</td>\n",
       "      <td>0.311531</td>\n",
       "      <td>0.404004</td>\n",
       "      <td>22.493883</td>\n",
       "      <td>107.951242</td>\n",
       "      <td>249.714723</td>\n",
       "      <td>0.589017</td>\n",
       "      <td>1.230617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clioquinol</td>\n",
       "      <td>Oc1c(I)cc(Cl)c2cccnc12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7041</td>\n",
       "      <td>2.903957</td>\n",
       "      <td>22.1264</td>\n",
       "      <td>28.605965</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636046</td>\n",
       "      <td>0.348308</td>\n",
       "      <td>0.488440</td>\n",
       "      <td>0.490672</td>\n",
       "      <td>0.207540</td>\n",
       "      <td>7.749546</td>\n",
       "      <td>14.229627</td>\n",
       "      <td>23.592414</td>\n",
       "      <td>0.476530</td>\n",
       "      <td>1.186652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uk-240,455</td>\n",
       "      <td>CS(=O)(=O)N(CCO)c1c(Cl)c(Cl)cc2[nH]c(=O)c(=O)[...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0523</td>\n",
       "      <td>1.107335</td>\n",
       "      <td>54.9415</td>\n",
       "      <td>41.264723</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.593743</td>\n",
       "      <td>0.304769</td>\n",
       "      <td>0.585940</td>\n",
       "      <td>0.486829</td>\n",
       "      <td>0.291021</td>\n",
       "      <td>11.609978</td>\n",
       "      <td>36.682528</td>\n",
       "      <td>77.031924</td>\n",
       "      <td>0.390615</td>\n",
       "      <td>1.363790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>l-701,324</td>\n",
       "      <td>O=c1[nH]c2cc(Cl)ccc2c(O)c1-c1cccc(Oc2ccccc2)c1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5049</td>\n",
       "      <td>0.254924</td>\n",
       "      <td>17.2408</td>\n",
       "      <td>51.981102</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.884187</td>\n",
       "      <td>0.086784</td>\n",
       "      <td>0.528627</td>\n",
       "      <td>0.533905</td>\n",
       "      <td>0.519028</td>\n",
       "      <td>23.901180</td>\n",
       "      <td>59.936899</td>\n",
       "      <td>114.251776</td>\n",
       "      <td>0.826281</td>\n",
       "      <td>1.581560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>icotidine</td>\n",
       "      <td>COc1cccnc1CCCCNc1ncc(Cc2ccc(C)nc2)c(=O)[nH]1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>0.270504</td>\n",
       "      <td>43.3780</td>\n",
       "      <td>60.733825</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.825394</td>\n",
       "      <td>0.137564</td>\n",
       "      <td>0.632738</td>\n",
       "      <td>0.490041</td>\n",
       "      <td>0.402539</td>\n",
       "      <td>35.819782</td>\n",
       "      <td>191.450811</td>\n",
       "      <td>420.570685</td>\n",
       "      <td>0.738091</td>\n",
       "      <td>1.525318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9574</th>\n",
       "      <td>xanomeline</td>\n",
       "      <td>C(OC1=NSN=C1C2=CCCN(C2)C)CCCCC</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.4141</td>\n",
       "      <td>0.171479</td>\n",
       "      <td>74.2319</td>\n",
       "      <td>46.978239</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.682468</td>\n",
       "      <td>0.248147</td>\n",
       "      <td>0.506367</td>\n",
       "      <td>0.371147</td>\n",
       "      <td>0.344447</td>\n",
       "      <td>15.605274</td>\n",
       "      <td>56.965977</td>\n",
       "      <td>117.226098</td>\n",
       "      <td>0.523702</td>\n",
       "      <td>1.221961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9575</th>\n",
       "      <td>zaleplon</td>\n",
       "      <td>C2=N[N]1C(=CC=NC1=C2C#N)C3=CC(=CC=C3)N(C(C)=O)CC</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6641</td>\n",
       "      <td>0.441029</td>\n",
       "      <td>92.8289</td>\n",
       "      <td>46.223895</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797113</td>\n",
       "      <td>0.129740</td>\n",
       "      <td>0.547931</td>\n",
       "      <td>0.390597</td>\n",
       "      <td>0.300298</td>\n",
       "      <td>17.383025</td>\n",
       "      <td>51.735648</td>\n",
       "      <td>108.853106</td>\n",
       "      <td>0.695670</td>\n",
       "      <td>1.238826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9577</th>\n",
       "      <td>zomebazam</td>\n",
       "      <td>C3=C(N2C1=C([N](C)N=C1C)N(C(=O)CC2=O)C)C=CC=C3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1258</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>83.7217</td>\n",
       "      <td>43.072688</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602056</td>\n",
       "      <td>0.290070</td>\n",
       "      <td>0.500983</td>\n",
       "      <td>0.491839</td>\n",
       "      <td>0.397837</td>\n",
       "      <td>11.828007</td>\n",
       "      <td>37.895949</td>\n",
       "      <td>80.897747</td>\n",
       "      <td>0.403084</td>\n",
       "      <td>1.390658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9578</th>\n",
       "      <td>zometapine</td>\n",
       "      <td>C3=C(C1=NCCN=C2N(NC(=C12)C)C)C=CC=C3Cl</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5159</td>\n",
       "      <td>0.266153</td>\n",
       "      <td>81.9598</td>\n",
       "      <td>41.221895</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553319</td>\n",
       "      <td>0.336731</td>\n",
       "      <td>0.456428</td>\n",
       "      <td>0.447273</td>\n",
       "      <td>0.419807</td>\n",
       "      <td>11.159667</td>\n",
       "      <td>35.391315</td>\n",
       "      <td>75.022207</td>\n",
       "      <td>0.335076</td>\n",
       "      <td>1.323508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9579</th>\n",
       "      <td>licostinel</td>\n",
       "      <td>C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1909</td>\n",
       "      <td>0.036443</td>\n",
       "      <td>57.1443</td>\n",
       "      <td>26.948379</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598857</td>\n",
       "      <td>0.350759</td>\n",
       "      <td>0.566435</td>\n",
       "      <td>0.535897</td>\n",
       "      <td>0.365741</td>\n",
       "      <td>8.796186</td>\n",
       "      <td>19.954489</td>\n",
       "      <td>35.953677</td>\n",
       "      <td>0.424423</td>\n",
       "      <td>1.468073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6312 rows × 1877 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Original_Name                                             smiles  nAcid  \\\n",
       "1       moxalactam  COC1(NC(=O)C(C(=O)O)c2ccc(O)cc2)C(=O)N2C(C(=O)...      4   \n",
       "2       clioquinol                             Oc1c(I)cc(Cl)c2cccnc12      0   \n",
       "5       uk-240,455  CS(=O)(=O)N(CCO)c1c(Cl)c(Cl)cc2[nH]c(=O)c(=O)[...      0   \n",
       "8        l-701,324     O=c1[nH]c2cc(Cl)ccc2c(O)c1-c1cccc(Oc2ccccc2)c1      0   \n",
       "10       icotidine       COc1cccnc1CCCCNc1ncc(Cc2ccc(C)nc2)c(=O)[nH]1      0   \n",
       "...            ...                                                ...    ...   \n",
       "9574    xanomeline                     C(OC1=NSN=C1C2=CCCN(C2)C)CCCCC      0   \n",
       "9575      zaleplon   C2=N[N]1C(=CC=NC1=C2C#N)C3=CC(=CC=C3)N(C(C)=O)CC      0   \n",
       "9577     zomebazam     C3=C(N2C1=C([N](C)N=C1C)N(C(=O)CC2=O)C)C=CC=C3      0   \n",
       "9578    zometapine             C3=C(C1=NCCN=C2N(NC(=C12)C)C)C=CC=C3Cl      0   \n",
       "9579    licostinel    C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl      0   \n",
       "\n",
       "       ALogP    ALogp2      AMR       apol  naAromAtom  nAromBond  nAtom  ...  \\\n",
       "1    -1.8532  3.434350  84.1596  65.253860          11         11     56  ...   \n",
       "2     1.7041  2.903957  22.1264  28.605965          10         11     18  ...   \n",
       "5    -1.0523  1.107335  54.9415  41.264723           6         11     33  ...   \n",
       "8    -0.5049  0.254924  17.2408  51.981102          18         24     40  ...   \n",
       "10   -0.5201  0.270504  43.3780  60.733825          12         18     53  ...   \n",
       "...      ...       ...      ...        ...         ...        ...    ...  ...   \n",
       "9574 -0.4141  0.171479  74.2319  46.978239           5          5     42  ...   \n",
       "9575  0.6641  0.441029  92.8289  46.223895          15         16     38  ...   \n",
       "9577 -0.1258  0.015826  83.7217  43.072688          11         11     37  ...   \n",
       "9578  0.5159  0.266153  81.9598  41.221895           6          6     34  ...   \n",
       "9579  0.1909  0.036443  57.1443  26.948379           6          6     20  ...   \n",
       "\n",
       "           P1s       P2s       E1s       E2s       E3s         Ts          As  \\\n",
       "1     0.726012  0.202821  0.515082  0.311531  0.404004  22.493883  107.951242   \n",
       "2     0.636046  0.348308  0.488440  0.490672  0.207540   7.749546   14.229627   \n",
       "5     0.593743  0.304769  0.585940  0.486829  0.291021  11.609978   36.682528   \n",
       "8     0.884187  0.086784  0.528627  0.533905  0.519028  23.901180   59.936899   \n",
       "10    0.825394  0.137564  0.632738  0.490041  0.402539  35.819782  191.450811   \n",
       "...        ...       ...       ...       ...       ...        ...         ...   \n",
       "9574  0.682468  0.248147  0.506367  0.371147  0.344447  15.605274   56.965977   \n",
       "9575  0.797113  0.129740  0.547931  0.390597  0.300298  17.383025   51.735648   \n",
       "9577  0.602056  0.290070  0.500983  0.491839  0.397837  11.828007   37.895949   \n",
       "9578  0.553319  0.336731  0.456428  0.447273  0.419807  11.159667   35.391315   \n",
       "9579  0.598857  0.350759  0.566435  0.535897  0.365741   8.796186   19.954489   \n",
       "\n",
       "              Vs        Ks        Ds  \n",
       "1     249.714723  0.589017  1.230617  \n",
       "2      23.592414  0.476530  1.186652  \n",
       "5      77.031924  0.390615  1.363790  \n",
       "8     114.251776  0.826281  1.581560  \n",
       "10    420.570685  0.738091  1.525318  \n",
       "...          ...       ...       ...  \n",
       "9574  117.226098  0.523702  1.221961  \n",
       "9575  108.853106  0.695670  1.238826  \n",
       "9577   80.897747  0.403084  1.390658  \n",
       "9578   75.022207  0.335076  1.323508  \n",
       "9579   35.953677  0.424423  1.468073  \n",
       "\n",
       "[6312 rows x 1877 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.rename(columns={'Original_SMILES':'smiles'}, inplace=True)\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb217e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>BBB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O=C(O)c1cc(N=Nc2ccc(S(=O)(=O)Nc3ccccn3)cc2)ccc1O</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COC1(NC(=O)C(C(=O)O)c2ccc(O)cc2)C(=O)N2C(C(=O)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oc1c(I)cc(Cl)c2cccnc12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCNC(=NCCSCc1ncccc1Br)NC#N</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CN1CC[C@]23c4c5ccc(OC6O[C@H](C(=O)O)[C@@H](O)[...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9632</th>\n",
       "      <td>C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9633</th>\n",
       "      <td>[C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9634</th>\n",
       "      <td>[O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9635</th>\n",
       "      <td>C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9636</th>\n",
       "      <td>[N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9637 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 smiles  BBB\n",
       "0      O=C(O)c1cc(N=Nc2ccc(S(=O)(=O)Nc3ccccn3)cc2)ccc1O    0\n",
       "1     COC1(NC(=O)C(C(=O)O)c2ccc(O)cc2)C(=O)N2C(C(=O)...    0\n",
       "2                                Oc1c(I)cc(Cl)c2cccnc12    0\n",
       "3                            CCNC(=NCCSCc1ncccc1Br)NC#N    0\n",
       "4     CN1CC[C@]23c4c5ccc(OC6O[C@H](C(=O)O)[C@@H](O)[...    0\n",
       "...                                                 ...  ...\n",
       "9632    C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl    1\n",
       "9633  [C@H]3([N]2C1=C(C(=NC=N1)N)N=C2)[C@@H]([C@@H](...    1\n",
       "9634  [O+]1=N[N](C=C1[N-]C(NC2=CC=CC=C2)=O)C(CC3=CC=...    1\n",
       "9635  C1=C(OC)C(=CC2=C1C(=[N+](C(=C2CC)C)[NH-])C3=CC...    1\n",
       "9636  [N+](=NCC(=O)N[C@@H]([C@H](O)C1=CC=C([N+]([O-]...    1\n",
       "\n",
       "[9637 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#appending BBB column to cleaned dataframe\n",
    "bbb_df = pd.read_csv('../data/combined_bbb_classification_fixed.csv')\n",
    "bbb_df = bbb_df[['smiles','BBB']]\n",
    "bbb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d9024ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sr/wdjmxmbx5lz8mb2nqmnbfvqm0000gp/T/ipykernel_73075/2193788045.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['BBB'] = None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original_Name</th>\n",
       "      <th>smiles</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>ALogP</th>\n",
       "      <th>ALogp2</th>\n",
       "      <th>AMR</th>\n",
       "      <th>apol</th>\n",
       "      <th>naAromAtom</th>\n",
       "      <th>nAromBond</th>\n",
       "      <th>nAtom</th>\n",
       "      <th>...</th>\n",
       "      <th>P2s</th>\n",
       "      <th>E1s</th>\n",
       "      <th>E2s</th>\n",
       "      <th>E3s</th>\n",
       "      <th>Ts</th>\n",
       "      <th>As</th>\n",
       "      <th>Vs</th>\n",
       "      <th>Ks</th>\n",
       "      <th>Ds</th>\n",
       "      <th>BBB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>moxalactam</td>\n",
       "      <td>COC1(NC(=O)C(C(=O)O)c2ccc(O)cc2)C(=O)N2C(C(=O)...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.8532</td>\n",
       "      <td>3.434350</td>\n",
       "      <td>84.1596</td>\n",
       "      <td>65.253860</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202821</td>\n",
       "      <td>0.515082</td>\n",
       "      <td>0.311531</td>\n",
       "      <td>0.404004</td>\n",
       "      <td>22.493883</td>\n",
       "      <td>107.951242</td>\n",
       "      <td>249.714723</td>\n",
       "      <td>0.589017</td>\n",
       "      <td>1.230617</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>clioquinol</td>\n",
       "      <td>Oc1c(I)cc(Cl)c2cccnc12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.7041</td>\n",
       "      <td>2.903957</td>\n",
       "      <td>22.1264</td>\n",
       "      <td>28.605965</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348308</td>\n",
       "      <td>0.488440</td>\n",
       "      <td>0.490672</td>\n",
       "      <td>0.207540</td>\n",
       "      <td>7.749546</td>\n",
       "      <td>14.229627</td>\n",
       "      <td>23.592414</td>\n",
       "      <td>0.476530</td>\n",
       "      <td>1.186652</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>uk-240,455</td>\n",
       "      <td>CS(=O)(=O)N(CCO)c1c(Cl)c(Cl)cc2[nH]c(=O)c(=O)[...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0523</td>\n",
       "      <td>1.107335</td>\n",
       "      <td>54.9415</td>\n",
       "      <td>41.264723</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304769</td>\n",
       "      <td>0.585940</td>\n",
       "      <td>0.486829</td>\n",
       "      <td>0.291021</td>\n",
       "      <td>11.609978</td>\n",
       "      <td>36.682528</td>\n",
       "      <td>77.031924</td>\n",
       "      <td>0.390615</td>\n",
       "      <td>1.363790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>l-701,324</td>\n",
       "      <td>O=c1[nH]c2cc(Cl)ccc2c(O)c1-c1cccc(Oc2ccccc2)c1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5049</td>\n",
       "      <td>0.254924</td>\n",
       "      <td>17.2408</td>\n",
       "      <td>51.981102</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086784</td>\n",
       "      <td>0.528627</td>\n",
       "      <td>0.533905</td>\n",
       "      <td>0.519028</td>\n",
       "      <td>23.901180</td>\n",
       "      <td>59.936899</td>\n",
       "      <td>114.251776</td>\n",
       "      <td>0.826281</td>\n",
       "      <td>1.581560</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>icotidine</td>\n",
       "      <td>COc1cccnc1CCCCNc1ncc(Cc2ccc(C)nc2)c(=O)[nH]1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5201</td>\n",
       "      <td>0.270504</td>\n",
       "      <td>43.3780</td>\n",
       "      <td>60.733825</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137564</td>\n",
       "      <td>0.632738</td>\n",
       "      <td>0.490041</td>\n",
       "      <td>0.402539</td>\n",
       "      <td>35.819782</td>\n",
       "      <td>191.450811</td>\n",
       "      <td>420.570685</td>\n",
       "      <td>0.738091</td>\n",
       "      <td>1.525318</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9574</th>\n",
       "      <td>xanomeline</td>\n",
       "      <td>C(OC1=NSN=C1C2=CCCN(C2)C)CCCCC</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.4141</td>\n",
       "      <td>0.171479</td>\n",
       "      <td>74.2319</td>\n",
       "      <td>46.978239</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248147</td>\n",
       "      <td>0.506367</td>\n",
       "      <td>0.371147</td>\n",
       "      <td>0.344447</td>\n",
       "      <td>15.605274</td>\n",
       "      <td>56.965977</td>\n",
       "      <td>117.226098</td>\n",
       "      <td>0.523702</td>\n",
       "      <td>1.221961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9575</th>\n",
       "      <td>zaleplon</td>\n",
       "      <td>C2=N[N]1C(=CC=NC1=C2C#N)C3=CC(=CC=C3)N(C(C)=O)CC</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6641</td>\n",
       "      <td>0.441029</td>\n",
       "      <td>92.8289</td>\n",
       "      <td>46.223895</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129740</td>\n",
       "      <td>0.547931</td>\n",
       "      <td>0.390597</td>\n",
       "      <td>0.300298</td>\n",
       "      <td>17.383025</td>\n",
       "      <td>51.735648</td>\n",
       "      <td>108.853106</td>\n",
       "      <td>0.695670</td>\n",
       "      <td>1.238826</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9577</th>\n",
       "      <td>zomebazam</td>\n",
       "      <td>C3=C(N2C1=C([N](C)N=C1C)N(C(=O)CC2=O)C)C=CC=C3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1258</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>83.7217</td>\n",
       "      <td>43.072688</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0.290070</td>\n",
       "      <td>0.500983</td>\n",
       "      <td>0.491839</td>\n",
       "      <td>0.397837</td>\n",
       "      <td>11.828007</td>\n",
       "      <td>37.895949</td>\n",
       "      <td>80.897747</td>\n",
       "      <td>0.403084</td>\n",
       "      <td>1.390658</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9578</th>\n",
       "      <td>zometapine</td>\n",
       "      <td>C3=C(C1=NCCN=C2N(NC(=C12)C)C)C=CC=C3Cl</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5159</td>\n",
       "      <td>0.266153</td>\n",
       "      <td>81.9598</td>\n",
       "      <td>41.221895</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>0.336731</td>\n",
       "      <td>0.456428</td>\n",
       "      <td>0.447273</td>\n",
       "      <td>0.419807</td>\n",
       "      <td>11.159667</td>\n",
       "      <td>35.391315</td>\n",
       "      <td>75.022207</td>\n",
       "      <td>0.335076</td>\n",
       "      <td>1.323508</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9579</th>\n",
       "      <td>licostinel</td>\n",
       "      <td>C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1909</td>\n",
       "      <td>0.036443</td>\n",
       "      <td>57.1443</td>\n",
       "      <td>26.948379</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350759</td>\n",
       "      <td>0.566435</td>\n",
       "      <td>0.535897</td>\n",
       "      <td>0.365741</td>\n",
       "      <td>8.796186</td>\n",
       "      <td>19.954489</td>\n",
       "      <td>35.953677</td>\n",
       "      <td>0.424423</td>\n",
       "      <td>1.468073</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6312 rows × 1878 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Original_Name                                             smiles  nAcid  \\\n",
       "1       moxalactam  COC1(NC(=O)C(C(=O)O)c2ccc(O)cc2)C(=O)N2C(C(=O)...      4   \n",
       "2       clioquinol                             Oc1c(I)cc(Cl)c2cccnc12      0   \n",
       "5       uk-240,455  CS(=O)(=O)N(CCO)c1c(Cl)c(Cl)cc2[nH]c(=O)c(=O)[...      0   \n",
       "8        l-701,324     O=c1[nH]c2cc(Cl)ccc2c(O)c1-c1cccc(Oc2ccccc2)c1      0   \n",
       "10       icotidine       COc1cccnc1CCCCNc1ncc(Cc2ccc(C)nc2)c(=O)[nH]1      0   \n",
       "...            ...                                                ...    ...   \n",
       "9574    xanomeline                     C(OC1=NSN=C1C2=CCCN(C2)C)CCCCC      0   \n",
       "9575      zaleplon   C2=N[N]1C(=CC=NC1=C2C#N)C3=CC(=CC=C3)N(C(C)=O)CC      0   \n",
       "9577     zomebazam     C3=C(N2C1=C([N](C)N=C1C)N(C(=O)CC2=O)C)C=CC=C3      0   \n",
       "9578    zometapine             C3=C(C1=NCCN=C2N(NC(=C12)C)C)C=CC=C3Cl      0   \n",
       "9579    licostinel    C1=C(Cl)C(=C(C2=C1NC(=O)C(N2)=O)[N+](=O)[O-])Cl      0   \n",
       "\n",
       "       ALogP    ALogp2      AMR       apol  naAromAtom  nAromBond  nAtom  ...  \\\n",
       "1    -1.8532  3.434350  84.1596  65.253860          11         11     56  ...   \n",
       "2     1.7041  2.903957  22.1264  28.605965          10         11     18  ...   \n",
       "5    -1.0523  1.107335  54.9415  41.264723           6         11     33  ...   \n",
       "8    -0.5049  0.254924  17.2408  51.981102          18         24     40  ...   \n",
       "10   -0.5201  0.270504  43.3780  60.733825          12         18     53  ...   \n",
       "...      ...       ...      ...        ...         ...        ...    ...  ...   \n",
       "9574 -0.4141  0.171479  74.2319  46.978239           5          5     42  ...   \n",
       "9575  0.6641  0.441029  92.8289  46.223895          15         16     38  ...   \n",
       "9577 -0.1258  0.015826  83.7217  43.072688          11         11     37  ...   \n",
       "9578  0.5159  0.266153  81.9598  41.221895           6          6     34  ...   \n",
       "9579  0.1909  0.036443  57.1443  26.948379           6          6     20  ...   \n",
       "\n",
       "           P2s       E1s       E2s       E3s         Ts          As  \\\n",
       "1     0.202821  0.515082  0.311531  0.404004  22.493883  107.951242   \n",
       "2     0.348308  0.488440  0.490672  0.207540   7.749546   14.229627   \n",
       "5     0.304769  0.585940  0.486829  0.291021  11.609978   36.682528   \n",
       "8     0.086784  0.528627  0.533905  0.519028  23.901180   59.936899   \n",
       "10    0.137564  0.632738  0.490041  0.402539  35.819782  191.450811   \n",
       "...        ...       ...       ...       ...        ...         ...   \n",
       "9574  0.248147  0.506367  0.371147  0.344447  15.605274   56.965977   \n",
       "9575  0.129740  0.547931  0.390597  0.300298  17.383025   51.735648   \n",
       "9577  0.290070  0.500983  0.491839  0.397837  11.828007   37.895949   \n",
       "9578  0.336731  0.456428  0.447273  0.419807  11.159667   35.391315   \n",
       "9579  0.350759  0.566435  0.535897  0.365741   8.796186   19.954489   \n",
       "\n",
       "              Vs        Ks        Ds  BBB  \n",
       "1     249.714723  0.589017  1.230617    0  \n",
       "2      23.592414  0.476530  1.186652    0  \n",
       "5      77.031924  0.390615  1.363790    0  \n",
       "8     114.251776  0.826281  1.581560    0  \n",
       "10    420.570685  0.738091  1.525318    0  \n",
       "...          ...       ...       ...  ...  \n",
       "9574  117.226098  0.523702  1.221961    1  \n",
       "9575  108.853106  0.695670  1.238826    1  \n",
       "9577   80.897747  0.403084  1.390658    1  \n",
       "9578   75.022207  0.335076  1.323508    1  \n",
       "9579   35.953677  0.424423  1.468073    1  \n",
       "\n",
       "[6312 rows x 1878 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['BBB'] = None\n",
    "for i in clean_df.index:\n",
    "    smile = clean_df.at[i,'smiles']\n",
    "    bbb_value = bbb_df.loc[bbb_df['smiles'] == smile, 'BBB']\n",
    "    if not bbb_value.empty:\n",
    "        clean_df.at[i, 'BBB'] = int(bbb_value.values[0])\n",
    "    else:\n",
    "        clean_df.at[i, 'BBB'] = None\n",
    "    \n",
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6309b602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BBB\n",
       "1    4568\n",
       "0    1744\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df['BBB'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fff0158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv('../data/padel_results_with_bbb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af351405",
   "metadata": {},
   "source": [
    "## Creating models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4551c2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for modeling and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_predict\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, matthews_corrcoef, average_precision_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('All libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7280106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (6312, 1878)\n",
      "\n",
      "First few rows:\n",
      "  Original_Name                                             smiles  nAcid  \\\n",
      "0    moxalactam  COC1(NC(=O)C(C(=O)O)c2ccc(O)cc2)C(=O)N2C(C(=O)...      4   \n",
      "1    clioquinol                             Oc1c(I)cc(Cl)c2cccnc12      0   \n",
      "2    uk-240,455  CS(=O)(=O)N(CCO)c1c(Cl)c(Cl)cc2[nH]c(=O)c(=O)[...      0   \n",
      "3     l-701,324     O=c1[nH]c2cc(Cl)ccc2c(O)c1-c1cccc(Oc2ccccc2)c1      0   \n",
      "4     icotidine       COc1cccnc1CCCCNc1ncc(Cc2ccc(C)nc2)c(=O)[nH]1      0   \n",
      "\n",
      "    ALogP    ALogp2      AMR       apol  naAromAtom  nAromBond  nAtom  ...  \\\n",
      "0 -1.8532  3.434350  84.1596  65.253860          11         11     56  ...   \n",
      "1  1.7041  2.903957  22.1264  28.605965          10         11     18  ...   \n",
      "2 -1.0523  1.107335  54.9415  41.264723           6         11     33  ...   \n",
      "3 -0.5049  0.254924  17.2408  51.981102          18         24     40  ...   \n",
      "4 -0.5201  0.270504  43.3780  60.733825          12         18     53  ...   \n",
      "\n",
      "        P2s       E1s       E2s       E3s         Ts          As          Vs  \\\n",
      "0  0.202821  0.515082  0.311531  0.404004  22.493883  107.951242  249.714723   \n",
      "1  0.348308  0.488440  0.490672  0.207540   7.749546   14.229627   23.592414   \n",
      "2  0.304769  0.585940  0.486829  0.291021  11.609978   36.682528   77.031924   \n",
      "3  0.086784  0.528627  0.533905  0.519028  23.901180   59.936899  114.251776   \n",
      "4  0.137564  0.632738  0.490041  0.402539  35.819782  191.450811  420.570685   \n",
      "\n",
      "         Ks        Ds  BBB  \n",
      "0  0.589017  1.230617    0  \n",
      "1  0.476530  1.186652    0  \n",
      "2  0.390615  1.363790    0  \n",
      "3  0.826281  1.581560    0  \n",
      "4  0.738091  1.525318    0  \n",
      "\n",
      "[5 rows x 1878 columns]\n",
      "\n",
      "Column names (first 10):\n",
      "['Original_Name', 'smiles', 'nAcid', 'ALogP', 'ALogp2', 'AMR', 'apol', 'naAromAtom', 'nAromBond', 'nAtom']\n"
     ]
    }
   ],
   "source": [
    "# Load the PADEL descriptors dataset with BBB values\n",
    "try:\n",
    "    padel_df = pd.read_csv('../data/padel_results_with_bbb.csv')\n",
    "    print(f'Dataset shape: {padel_df.shape}')\n",
    "    print(f'\\nFirst few rows:')\n",
    "    print(padel_df.head())\n",
    "    print(f'\\nColumn names (first 10):')\n",
    "    print(padel_df.columns[:10].tolist())\n",
    "except Exception as e:\n",
    "    print(f'Error loading data: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d516a438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in dataset:\n",
      "Original_Name    0\n",
      "smiles           0\n",
      "nAcid            0\n",
      "ALogP            0\n",
      "ALogp2           0\n",
      "                ..\n",
      "As               0\n",
      "Vs               0\n",
      "Ks               0\n",
      "Ds               0\n",
      "BBB              0\n",
      "Length: 1878, dtype: int64\n",
      "\n",
      "Dataset shape after dropping missing BBB: (6312, 1878)\n",
      "\n",
      "Number of descriptors: 1875\n",
      "First 10 descriptors: ['nAcid', 'ALogP', 'ALogp2', 'AMR', 'apol', 'naAromAtom', 'nAromBond', 'nAtom', 'nHeavyAtom', 'nH']\n",
      "Last 10 descriptors: ['P1s', 'P2s', 'E1s', 'E2s', 'E3s', 'Ts', 'As', 'Vs', 'Ks', 'Ds']\n",
      "\n",
      "BBB value distribution:\n",
      "BBB\n",
      "0    1744\n",
      "1    4568\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Check for missing values\n",
    "    print(f'Missing values in dataset:')\n",
    "    print(padel_df.isna().sum())\n",
    "    \n",
    "    # Drop rows with missing BBB values\n",
    "    padel_df_clean = padel_df.dropna(subset=['BBB']).copy()\n",
    "    print(f'\\nDataset shape after dropping missing BBB: {padel_df_clean.shape}')\n",
    "    \n",
    "    # Drop non-descriptor columns (Original_Name and smiles)\n",
    "    # Descriptors start from 'nAcid' onwards\n",
    "    descriptor_start_idx = padel_df_clean.columns.tolist().index('nAcid')\n",
    "    descriptor_cols = padel_df_clean.columns[descriptor_start_idx:-1].tolist()  # Exclude BBB column\n",
    "    \n",
    "    print(f'\\nNumber of descriptors: {len(descriptor_cols)}')\n",
    "    print(f'First 10 descriptors: {descriptor_cols[:10]}')\n",
    "    print(f'Last 10 descriptors: {descriptor_cols[-10:]}')\n",
    "    \n",
    "    # Check BBB distribution\n",
    "    print(f'\\nBBB value distribution:')\n",
    "    print(padel_df_clean['BBB'].value_counts().sort_index())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'Error in data preparation: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "903148bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (6312, 1875)\n",
      "Target vector shape: (6312,)\n",
      "\n",
      "Missing values after filling:\n",
      "0\n",
      "\n",
      "Scaled features shape: (6312, 1875)\n",
      "Scaled features - mean close to 0: -0.000000\n",
      "Scaled features - std close to 1: 0.884870\n",
      "Scaled features - std close to 1: 0.884870\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Fill any remaining missing values with zero in descriptor columns\n",
    "    X = padel_df_clean[descriptor_cols].fillna(0)\n",
    "    y = padel_df_clean['BBB']\n",
    "    \n",
    "    print(f'Feature matrix shape: {X.shape}')\n",
    "    print(f'Target vector shape: {y.shape}')\n",
    "    print(f'\\nMissing values after filling:')\n",
    "    print(X.isna().sum().sum())\n",
    "    \n",
    "    # Standardize the features (important for distance-based algorithms like KNN and SVM)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Create a DataFrame with scaled features for easier manipulation\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=descriptor_cols, index=X.index)\n",
    "    \n",
    "    print(f'\\nScaled features shape: {X_scaled_df.shape}')\n",
    "    print(f'Scaled features - mean close to 0: {X_scaled_df.mean().mean():.6f}')\n",
    "    print(f'Scaled features - std close to 1: {X_scaled_df.std().mean():.6f}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'Error in feature standardization: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6ec7716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset shape: (9136, 1876)\n",
      "\n",
      "Balanced BBB distribution:\n",
      "BBB\n",
      "0    4568\n",
      "1    4568\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Plot class imbalance before balancing\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Before SMOTE\n",
    "    sns.countplot(x='BBB', data=pd.DataFrame({'BBB': y}), palette='Set1', ax=axes[0])\n",
    "    axes[0].set_title('BBB Class Distribution (Before SMOTE)')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    \n",
    "    # Apply SMOTE to balance the classes\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_res, y_res = smote.fit_resample(X_scaled_df, y)\n",
    "    \n",
    "    # After SMOTE\n",
    "    sns.countplot(x='BBB', data=pd.DataFrame({'BBB': y_res}), palette='Set1', ax=axes[1])\n",
    "    axes[1].set_title('BBB Class Distribution (After SMOTE)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/padel_class_distribution_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create balanced DataFrame\n",
    "    balanced_padel_df = pd.DataFrame(X_res, columns=descriptor_cols)\n",
    "    balanced_padel_df['BBB'] = y_res.values\n",
    "    \n",
    "    print(f'Balanced dataset shape: {balanced_padel_df.shape}')\n",
    "    print(f'\\nBalanced BBB distribution:')\n",
    "    print(balanced_padel_df['BBB'].value_counts().sort_index())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'Error in class balancing: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d64eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (7308, 1876)\n",
      "Test set shape: (1828, 1876)\n",
      "\n",
      "Training set BBB distribution:\n",
      "BBB\n",
      "0    3654\n",
      "1    3654\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test set BBB distribution:\n",
      "BBB\n",
      "0    914\n",
      "1    914\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Split balanced data into train and test sets (80-20 split)\n",
    "    train_padel_df, test_padel_df = train_test_split(\n",
    "        balanced_padel_df, \n",
    "        test_size=0.2, \n",
    "        random_state=42,\n",
    "        stratify=balanced_padel_df['BBB']  # Ensure balanced split\n",
    "    )\n",
    "    \n",
    "    print(f'Training set shape: {train_padel_df.shape}')\n",
    "    print(f'Test set shape: {test_padel_df.shape}')\n",
    "    print(f'\\nTraining set BBB distribution:')\n",
    "    print(train_padel_df['BBB'].value_counts().sort_index())\n",
    "    print(f'\\nTest set BBB distribution:')\n",
    "    print(test_padel_df['BBB'].value_counts().sort_index())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'Error in data splitting: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78c2d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LazyPredict imported successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # First, try to import LazyPredict; if not available, install it\n",
    "    try:\n",
    "        from lazypredict.Supervised import LazyClassifier\n",
    "    except ImportError:\n",
    "        import subprocess\n",
    "        print('Installing lazypredict...')\n",
    "        subprocess.check_call(['pip', 'install', 'lazypredict'])\n",
    "        from lazypredict.Supervised import LazyClassifier\n",
    "    \n",
    "    print('LazyPredict imported successfully!')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'Error with LazyPredict setup: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0c667e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LazyPredict classification...\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dec0e1a8c4544688625ba15c0737204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.051905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376752\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "=== LazyPredict Results (Top 10 Models by Accuracy) ===\n",
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "ExtraTreesClassifier               0.94               0.94     0.94      0.94   \n",
      "XGBClassifier                      0.94               0.94     0.94      0.94   \n",
      "LGBMClassifier                     0.94               0.94     0.94      0.94   \n",
      "RandomForestClassifier             0.94               0.94     0.94      0.94   \n",
      "QuadraticDiscriminantAnalysis      0.92               0.92     0.92      0.92   \n",
      "BaggingClassifier                  0.92               0.92     0.92      0.92   \n",
      "LinearDiscriminantAnalysis         0.90               0.90     0.90      0.90   \n",
      "SVC                                0.90               0.90     0.90      0.90   \n",
      "LinearSVC                          0.90               0.90     0.90      0.90   \n",
      "CalibratedClassifierCV             0.90               0.90     0.90      0.90   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "ExtraTreesClassifier                 3.01  \n",
      "XGBClassifier                        8.84  \n",
      "LGBMClassifier                       6.04  \n",
      "RandomForestClassifier              14.15  \n",
      "QuadraticDiscriminantAnalysis       12.03  \n",
      "BaggingClassifier                   86.38  \n",
      "LinearDiscriminantAnalysis           8.92  \n",
      "SVC                                 13.57  \n",
      "LinearSVC                           63.07  \n",
      "CalibratedClassifierCV             250.51  \n",
      "\n",
      "=== LazyPredict Results (Top 10 Models by Accuracy) ===\n",
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "ExtraTreesClassifier               0.94               0.94     0.94      0.94   \n",
      "XGBClassifier                      0.94               0.94     0.94      0.94   \n",
      "LGBMClassifier                     0.94               0.94     0.94      0.94   \n",
      "RandomForestClassifier             0.94               0.94     0.94      0.94   \n",
      "QuadraticDiscriminantAnalysis      0.92               0.92     0.92      0.92   \n",
      "BaggingClassifier                  0.92               0.92     0.92      0.92   \n",
      "LinearDiscriminantAnalysis         0.90               0.90     0.90      0.90   \n",
      "SVC                                0.90               0.90     0.90      0.90   \n",
      "LinearSVC                          0.90               0.90     0.90      0.90   \n",
      "CalibratedClassifierCV             0.90               0.90     0.90      0.90   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "ExtraTreesClassifier                 3.01  \n",
      "XGBClassifier                        8.84  \n",
      "LGBMClassifier                       6.04  \n",
      "RandomForestClassifier              14.15  \n",
      "QuadraticDiscriminantAnalysis       12.03  \n",
      "BaggingClassifier                   86.38  \n",
      "LinearDiscriminantAnalysis           8.92  \n",
      "SVC                                 13.57  \n",
      "LinearSVC                           63.07  \n",
      "CalibratedClassifierCV             250.51  \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Function to run lazy classifier\n",
    "    def run_lazy_classifier_padel(data):\n",
    "        \"\"\"\n",
    "        Run LazyPredict on the PADEL data to identify promising models.\n",
    "        \n",
    "        Design Choice: Using LazyPredict for initial model screening allows us to:\n",
    "        1. Quickly benchmark multiple algorithms without manual tuning\n",
    "        2. Identify which model families (tree-based, linear, distance-based) work best\n",
    "        3. Focus hyperparameter tuning on the most promising candidates\n",
    "        \"\"\"\n",
    "        X = data.drop(columns='BBB')\n",
    "        y = data['BBB']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        print('Running LazyPredict classification...')\n",
    "        print('This may take a few minutes...\\n')\n",
    "        \n",
    "        clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "        models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        return models.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    # Run LazyPredict\n",
    "    lazy_results = run_lazy_classifier_padel(train_padel_df)\n",
    "    print('\\n=== LazyPredict Results (Top 10 Models by Accuracy) ===')\n",
    "    print(lazy_results.head(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'Error running LazyPredict: {e}')\n",
    "    print('This is expected if lazypredict has dependency issues.')\n",
    "    print('We will proceed with selected models anyway.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "affaabf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fce474cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline and GridSearchCV imported!\n",
      "\n",
      "--- Tuning KNN ---\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "\n",
      "--- Tuning KNN ---\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best params for KNN: {'clf__metric': 'manhattan', 'clf__n_neighbors': 3, 'clf__weights': 'distance'}\n",
      "Best Accuracy (CV): 0.9271\n",
      "\n",
      "--- Tuning LR ---\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best params for KNN: {'clf__metric': 'manhattan', 'clf__n_neighbors': 3, 'clf__weights': 'distance'}\n",
      "Best Accuracy (CV): 0.9271\n",
      "\n",
      "--- Tuning LR ---\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best params for LR: {'clf__C': 10, 'clf__penalty': 'l2'}\n",
      "Best Accuracy (CV): 0.9024\n",
      "\n",
      "--- Tuning RF ---\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best params for LR: {'clf__C': 10, 'clf__penalty': 'l2'}\n",
      "Best Accuracy (CV): 0.9024\n",
      "\n",
      "--- Tuning RF ---\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best params for RF: {'max_depth': 16, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Accuracy (CV): 0.9293\n",
      "\n",
      "--- Tuning ET ---\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best params for RF: {'max_depth': 16, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Accuracy (CV): 0.9293\n",
      "\n",
      "--- Tuning ET ---\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best params for ET: {'max_depth': 16, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Accuracy (CV): 0.9309\n",
      "\n",
      "--- Tuning LGBM ---\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best params for ET: {'max_depth': 16, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Accuracy (CV): 0.9309\n",
      "\n",
      "--- Tuning LGBM ---\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.534809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.536630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.534809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.536630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.950706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.950706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.906663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.906663 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.057433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.062113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.057433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.062113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.818553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.818553 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.879164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.879164 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.778456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.778456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.156720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.156720 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.792475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.792475 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.864465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.864465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.612363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.612363 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.415605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.415605 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.474216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.474216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.877102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.877102 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.390514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.390514 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.802680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.802680 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.395035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.395035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.276433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.276433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.691415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.691415 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.484175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.484175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.737423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.737423 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.798350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.798350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.290495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.290495 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.637517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.637517 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.804652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.804652 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.676742 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.676742 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.871305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.871305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.317856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.317856 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.687587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.687587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.879025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.879025 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.719358 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.719358 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.878301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.878301 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.725144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.725144 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.735425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.735425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.745011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.745011 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.249031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.249031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.666934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.666934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.522409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.522409 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Number of positive: 3654, number of negative: 3654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 379683\n",
      "[LightGBM] [Info] Number of positive: 3654, number of negative: 3654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.107452 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 379683\n",
      "[LightGBM] [Info] Number of data points in the train set: 7308, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 7308, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Best params for LGBM: {'learning_rate': 0.05, 'n_estimators': 200, 'num_leaves': 63}\n",
      "Best Accuracy (CV): 0.9379\n",
      "Best params for LGBM: {'learning_rate': 0.05, 'n_estimators': 200, 'num_leaves': 63}\n",
      "Best Accuracy (CV): 0.9379\n",
      "Retraining KNN on full dataset for saving...\n",
      "Retraining KNN on full dataset for saving...\n",
      "Retraining LR on full dataset for saving...\n",
      "Retraining LR on full dataset for saving...\n",
      "Retraining RF on full dataset for saving...\n",
      "Retraining RF on full dataset for saving...\n",
      "Retraining ET on full dataset for saving...\n",
      "Retraining ET on full dataset for saving...\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.129877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080617 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.118814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.167751 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.134208 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377071\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.164113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Total Bins 377048\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147194 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.147194 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Total Bins 376773\n",
      "[LightGBM] [Info] Number of data points in the train set: 5846, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] Number of positive: 2924, number of negative: 2923\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.120695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 376843\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500086 -> initscore=0.000342\n",
      "[LightGBM] [Info] Start training from score 0.000342\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "[LightGBM] [Info] Number of positive: 2923, number of negative: 2924\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086171 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 377042\n",
      "[LightGBM] [Info] Number of data points in the train set: 5847, number of used features: 1628\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499914 -> initscore=-0.000342\n",
      "[LightGBM] [Info] Start training from score -0.000342\n",
      "Retraining LGBM on full dataset for saving...\n",
      "Retraining LGBM on full dataset for saving...\n",
      "[LightGBM] [Info] Number of positive: 3654, number of negative: 3654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Number of positive: 3654, number of negative: 3654\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.155734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 379683\n",
      "[LightGBM] [Info] Number of data points in the train set: 7308, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Info] Total Bins 379683\n",
      "[LightGBM] [Info] Number of data points in the train set: 7308, number of used features: 1632\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "\n",
      "All models tuned, retrained, and saved!\n",
      "\n",
      "All models tuned, retrained, and saved!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>BestParams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.95</td>\n",
       "      <td>{'clf__metric': 'manhattan', 'clf__n_neighbors...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.92</td>\n",
       "      <td>{'clf__C': 10, 'clf__penalty': 'l2'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.92</td>\n",
       "      <td>{'max_depth': 16, 'min_samples_split': 2, 'n_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ET</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>{'max_depth': 16, 'min_samples_split': 2, 'n_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.93</td>\n",
       "      <td>{'learning_rate': 0.05, 'n_estimators': 200, '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model  Accuracy  F1 Score  roc_auc  Precision  Recall  Specificity  \\\n",
       "0   KNN      0.93      0.93     0.96       0.95    0.91         0.95   \n",
       "1    LR      0.90      0.90     0.95       0.92    0.89         0.92   \n",
       "2    RF      0.93      0.93     0.98       0.92    0.94         0.92   \n",
       "3    ET      0.93      0.93     0.98       0.93    0.94         0.93   \n",
       "4  LGBM      0.94      0.94     0.98       0.93    0.94         0.93   \n",
       "\n",
       "                                          BestParams  \n",
       "0  {'clf__metric': 'manhattan', 'clf__n_neighbors...  \n",
       "1               {'clf__C': 10, 'clf__penalty': 'l2'}  \n",
       "2  {'max_depth': 16, 'min_samples_split': 2, 'n_e...  \n",
       "3  {'max_depth': 16, 'min_samples_split': 2, 'n_e...  \n",
       "4  {'learning_rate': 0.05, 'n_estimators': 200, '...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix, matthews_corrcoef\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "\n",
    "    print('Pipeline and GridSearchCV imported!')\n",
    "\n",
    "    # Model definitions and grids\n",
    "    model_grids = [\n",
    "        # KNN\n",
    "        ('KNN', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('clf', KNeighborsClassifier())\n",
    "        ]), {\n",
    "            'clf__n_neighbors': [3, 5, 7],\n",
    "            'clf__weights': ['uniform', 'distance'],\n",
    "            'clf__metric': ['euclidean', 'manhattan']\n",
    "        }),\n",
    "        # # SVM\n",
    "        # ('SVM', Pipeline([\n",
    "        #     ('scaler', StandardScaler()),\n",
    "        #     ('clf', SVC(probability=True, random_state=42))\n",
    "        # ]), {\n",
    "        #     'clf__C': [0.1, 1, 10],\n",
    "        #     'clf__kernel': ['rbf', 'linear']\n",
    "        # }),\n",
    "        # Logistic Regression\n",
    "        ('LR', Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('clf', LogisticRegression(max_iter=1000, random_state=42))\n",
    "        ]), {\n",
    "            'clf__C': [0.1, 1, 10],\n",
    "            'clf__penalty': ['l2']\n",
    "        }),\n",
    "        # Random Forest\n",
    "        ('RF', RandomForestClassifier(random_state=42), {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [8, 16],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }),\n",
    "        # Extra Trees\n",
    "        ('ET', ExtraTreesClassifier(random_state=42), {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [8, 16],\n",
    "            'min_samples_split': [2, 5]\n",
    "        }),\n",
    "        # LightGBM\n",
    "        ('LGBM', LGBMClassifier(random_state=42), {\n",
    "            'n_estimators': [100, 200],\n",
    "            'num_leaves': [31, 63],\n",
    "            'learning_rate': [0.05, 0.1]\n",
    "        })\n",
    "    ]\n",
    "\n",
    "    # Prepare data\n",
    "    X_train = train_padel_df.drop(columns='BBB')\n",
    "    y_train = train_padel_df['BBB']\n",
    "\n",
    "    best_models = {}\n",
    "    best_params = {}\n",
    "    best_scores = {}\n",
    "    metrics_results = []\n",
    "\n",
    "    for name, model, param_grid in model_grids:\n",
    "        print(f'\\n--- Tuning {name} ---')\n",
    "        gs = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "        gs.fit(X_train, y_train)\n",
    "        print(f'Best params for {name}: {gs.best_params_}')\n",
    "        print(f'Best Accuracy (CV): {gs.best_score_:.4f}')\n",
    "        best_models[name] = gs.best_estimator_\n",
    "        best_params[name] = gs.best_params_\n",
    "        best_scores[name] = gs.best_score_\n",
    "\n",
    "    # Retrain best models on full training data, save, and calculate metrics\n",
    "    for name, model in best_models.items():\n",
    "\n",
    "        # Using cross_val_predict for HONEST metrics\n",
    "        # This simulates \"unseen\" data for the report card\n",
    "        y_pred = cross_val_predict(model, X_train, y_train, cv=5, n_jobs=1)\n",
    "        \n",
    "        # Handle probabilities for AUC\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            # For pipelines, we need to access the probability method carefully or use cross_val_predict method\n",
    "            try:\n",
    "                y_proba = cross_val_predict(model, X_train, y_train, cv=5, method='predict_proba', n_jobs=1)[:, 1]\n",
    "            except:\n",
    "                # Fallback if method fails (rare)\n",
    "                y_proba = [0] * len(y_train)\n",
    "        else:\n",
    "             y_proba = [0] * len(y_train)\n",
    "\n",
    "        # Calculate Metrics based on the CROSS-VALIDATED predictions\n",
    "        cm = confusion_matrix(y_train, y_pred)\n",
    "        TN, FP, FN, TP = cm.ravel()\n",
    "        specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "\n",
    "        metrics_results.append({\n",
    "            'Model': name,\n",
    "            'Accuracy': accuracy_score(y_train, y_pred),\n",
    "            'Precision': precision_score(y_train, y_pred),\n",
    "            'Recall': recall_score(y_train, y_pred),\n",
    "            'Specificity': specificity,\n",
    "            'F1 Score': f1_score(y_train, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_train, y_proba) if len(set(y_train)) > 1 else 0,\n",
    "            'BestParams': str(best_params[name]) # Convert dict to string for CSV safety\n",
    "        })\n",
    "\n",
    "        # Final Step: Retrain on 100% data for saving (Production Ready)\n",
    "        print(f'Retraining {name} on full dataset for saving...')\n",
    "        model.fit(X_train, y_train)\n",
    "        joblib.dump(model, f'../output/models/padel_{name}_model.pkl')\n",
    "\n",
    "    # Save metrics to CSV\n",
    "    metrics_df = pd.DataFrame(metrics_results)\n",
    "    # Reorder columns for readability\n",
    "    cols = ['Model', 'Accuracy', 'F1 Score', 'roc_auc', 'Precision', 'Recall', 'Specificity', 'BestParams']\n",
    "    metrics_df = metrics_df[cols]\n",
    "    \n",
    "    metrics_df.to_csv('../output/models/padel_train_metrics_summary.csv', index=False)\n",
    "    \n",
    "    print('\\nAll models tuned, retrained, and saved!')\n",
    "    display(metrics_df)\n",
    "except Exception as e:\n",
    "    print(f'Error in refactored model training pipeline: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7866eac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'KNN': KNeighborsClassifier(n_neighbors=3), 'SVM': SVC(probability=True, random_state=42), 'RF': RandomForestClassifier(max_depth=8, random_state=42), 'LR': LogisticRegression(max_iter=1000, random_state=42), 'ET': ExtraTreesClassifier(random_state=42), 'LGBM': LGBMClassifier(random_state=42)}\n"
     ]
    }
   ],
   "source": [
    "print(padel_saved_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5ec6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training XGBoost ---\n",
      "Error training XGBoost: 'DataFrame' object has no attribute 'append'\n",
      "Error training XGBoost: 'DataFrame' object has no attribute 'append'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from sklearn.model_selection import GridSearchCV, cross_val_predict\n",
    "    from xgboost import XGBClassifier\n",
    "    import joblib\n",
    "    import pandas as pd\n",
    "    print('GridSearchCV and XGBClassifier imported!')\n",
    "\n",
    "    X_train = train_padel_df.drop(columns='BBB')\n",
    "    y_train = train_padel_df['BBB']\n",
    "\n",
    "    # XGBoost hyperparameter grid\n",
    "    xgb_param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [4, 8],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "    gs_xgb = GridSearchCV(xgb, xgb_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    gs_xgb.fit(X_train, y_train)\n",
    "    print(f'Best params for XGB: {gs_xgb.best_params_}')\n",
    "    print(f'Best Accuracy (CV): {gs_xgb.best_score_:.4f}')\n",
    "    best_xgb = gs_xgb.best_estimator_\n",
    "\n",
    "    # Cross-validated metrics for XGB\n",
    "    y_pred_xgb = cross_val_predict(best_xgb, X_train, y_train, cv=5, n_jobs=1)\n",
    "    y_proba_xgb = cross_val_predict(best_xgb, X_train, y_train, cv=5, method='predict_proba', n_jobs=1)[:, 1]\n",
    "    cm_xgb = confusion_matrix(y_train, y_pred_xgb)\n",
    "    TN, FP, FN, TP = cm_xgb.ravel()\n",
    "    specificity_xgb = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "\n",
    "    # Append to metrics_results (created in previous cell)\n",
    "    metrics_results.append({\n",
    "        'Model': 'XGB',\n",
    "        'Accuracy': accuracy_score(y_train, y_pred_xgb),\n",
    "        'Precision': precision_score(y_train, y_pred_xgb),\n",
    "        'Recall': recall_score(y_train, y_pred_xgb),\n",
    "        'Specificity': specificity_xgb,\n",
    "        'F1 Score': f1_score(y_train, y_pred_xgb),\n",
    "        'roc_auc': roc_auc_score(y_train, y_proba_xgb),\n",
    "        'BestParams': str(gs_xgb.best_params_)\n",
    "    })\n",
    "\n",
    "    # Retrain on full data and save\n",
    "    best_xgb.fit(X_train, y_train)\n",
    "    joblib.dump(best_xgb, '../output/models/padel_XGB_model.pkl')\n",
    "    print('Saved XGB model to ../output/models/padel_XGB_model.pkl')\n",
    "\n",
    "    # Save all metrics to CSV (including XGB)\n",
    "    metrics_df = pd.DataFrame(metrics_results)\n",
    "    cols = ['Model', 'Accuracy', 'F1 Score', 'roc_auc', 'Precision', 'Recall', 'Specificity', 'BestParams']\n",
    "    metrics_df = metrics_df[cols]\n",
    "    metrics_df.to_csv('../output/models/padel_train_metrics_summary.csv', index=False)\n",
    "    print('Updated metrics summary saved to ../output/models/padel_train_metrics_summary.csv')\n",
    "    display(metrics_df)\n",
    "except Exception as e:\n",
    "    print(f'Error in XGBoost training and evaluation: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54cf15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create results DataFrame\n",
    "    padel_results_df = pd.DataFrame(padel_results)\n",
    "    padel_results_df = padel_results_df.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    print('\\nCROSS-VALIDATION RESULTS SUMMARY:')\n",
    "    print(padel_results_df.to_string())\n",
    "    \n",
    "    # Visualize metrics comparison\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1 Score', 'roc_auc', 'pr_auc', 'mcc']\n",
    "    \n",
    "    fig, axs = plt.subplots(3, 3, figsize=(16, 12))\n",
    "    fig.suptitle('PADEL Models - Metric Comparison (Cross-Validation)', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        \n",
    "        sns.barplot(\n",
    "            x='Model', y=metric, data=padel_results_df, \n",
    "            palette='Set2', ax=axs[row, col]\n",
    "        )\n",
    "        axs[row, col].set_title(metric, fontweight='bold')\n",
    "        axs[row, col].set_ylim(0, 1)\n",
    "        axs[row, col].set_ylabel(metric)\n",
    "        axs[row, col].set_xlabel('Model')\n",
    "        axs[row, col].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Remove the empty subplot\n",
    "    fig.delaxes(axs[2, 2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/padel_model_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'Error creating results visualization: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bff1a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d7019",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('\\n' + '='*80)\n",
    "    print('EVALUATING OPTIMIZED MODELS ON TEST SET')\n",
    "    print('='*80)\n",
    "    \n",
    "    X_test = test_padel_df.drop(columns='BBB')\n",
    "    y_test = test_padel_df['BBB']\n",
    "    \n",
    "    test_results = []\n",
    "    test_predictions = pd.DataFrame()\n",
    "    \n",
    "    # Evaluate all tuned models (including XGB)\n",
    "    all_models = dict(best_models)\n",
    "    all_models['XGB'] = best_xgb\n",
    "    \n",
    "    for name, model in all_models.items():\n",
    "        try:\n",
    "            y_pred = model.predict(X_test)\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_proba = model.predict_proba(X_test)[:, 1]\n",
    "            else:\n",
    "                y_proba = [0] * len(y_test)\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            TN, FP, FN, TP = cm.ravel()\n",
    "            specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n",
    "            test_results.append({\n",
    "                'Model': name,\n",
    "                'Accuracy': accuracy_score(y_test, y_pred),\n",
    "                'Precision': precision_score(y_test, y_pred),\n",
    "                'Recall': recall_score(y_test, y_pred),\n",
    "                'Specificity': specificity,\n",
    "                'F1 Score': f1_score(y_test, y_pred),\n",
    "                'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "            })\n",
    "            test_predictions[name] = y_pred\n",
    "            test_predictions[f'{name}_proba'] = y_proba\n",
    "            print(f'✓ {name} evaluated on test set')\n",
    "        except Exception as e:\n",
    "            print(f'Error evaluating {name} on test set: {e}')\n",
    "            continue\n",
    "    \n",
    "    # Save test results\n",
    "    test_results_df = pd.DataFrame(test_results)\n",
    "    cols = ['Model', 'Accuracy', 'F1 Score', 'roc_auc', 'Precision', 'Recall', 'Specificity']\n",
    "    test_results_df = test_results_df[cols]\n",
    "    test_results_df.to_csv('../output/models/padel_test_metrics_summary.csv', index=False)\n",
    "    print('Test set metrics summary saved to ../output/models/padel_test_metrics_summary.csv')\n",
    "    display(test_results_df)\n",
    "    \n",
    "    # Save predictions\n",
    "    test_predictions['BBB_actual'] = y_test.values\n",
    "    test_predictions.to_csv('../output/models/padel_test_predictions.csv', index=False)\n",
    "    print('Test set predictions saved to ../output/models/padel_test_predictions.csv')\n",
    "    \n",
    "    # Provide download links (for Jupyter/Colab)\n",
    "    from IPython.display import FileLink, display as ipydisplay\n",
    "    print('Download links:')\n",
    "    ipydisplay(FileLink('../output/models/padel_train_metrics_summary.csv'))\n",
    "    ipydisplay(FileLink('../output/models/padel_test_metrics_summary.csv'))\n",
    "    ipydisplay(FileLink('../output/models/padel_test_predictions.csv'))\n",
    "    for name in all_models:\n",
    "        ipydisplay(FileLink(f'../output/models/padel_{name}_model.pkl'))\n",
    "except Exception as e:\n",
    "    print(f'Error in test set evaluation and export: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'Specificity', 'F1 Score', 'roc_auc']\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(16, 8))\n",
    "    fig.suptitle('PADEL Models - Test Set Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    for i, metric in enumerate(metrics):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        sns.barplot(\n",
    "            x='Model', y=metric, data=test_results_df, \n",
    "            palette='husl', ax=axs[row, col]\n",
    "        )\n",
    "        axs[row, col].set_title(f'{metric} (Test Set)', fontweight='bold')\n",
    "        axs[row, col].set_ylim(0, 1)\n",
    "        axs[row, col].set_ylabel(metric)\n",
    "        axs[row, col].set_xlabel('Model')\n",
    "        axs[row, col].tick_params(axis='x', rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/padel_test_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f'Error visualizing test metrics: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b6e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print('\\n' + '='*80)\n",
    "    print('PREDICTION ANALYSIS ON BBB CLASSES (TEST SET)')\n",
    "    print('='*80)\n",
    "    model_names = list(test_results_df['Model'])\n",
    "    # BBB Positive cases\n",
    "    print('\\n--- BBB POSITIVE Cases Correctly Predicted by Each Model ---')\n",
    "    bbb_pos_correct = {}\n",
    "    total_bbb_pos = (test_predictions['BBB_actual'] == 1).sum()\n",
    "    print(f'Total BBB positive cases in test set: {total_bbb_pos}')\n",
    "    for model in model_names:\n",
    "        correct = ((test_predictions['BBB_actual'] == 1) & (test_predictions[model] == 1)).sum()\n",
    "        bbb_pos_correct[model] = correct\n",
    "        percentage = (correct / total_bbb_pos * 100) if total_bbb_pos > 0 else 0\n",
    "        print(f'{model}: {correct}/{total_bbb_pos} ({percentage:.1f}%)')\n",
    "    # BBB Negative cases\n",
    "    print('\\n--- BBB NEGATIVE Cases Correctly Predicted by Each Model ---')\n",
    "    bbb_neg_correct = {}\n",
    "    total_bbb_neg = (test_predictions['BBB_actual'] == 0).sum()\n",
    "    print(f'Total BBB negative cases in test set: {total_bbb_neg}')\n",
    "    for model in model_names:\n",
    "        correct = ((test_predictions['BBB_actual'] == 0) & (test_predictions[model] == 0)).sum()\n",
    "        bbb_neg_correct[model] = correct\n",
    "        percentage = (correct / total_bbb_neg * 100) if total_bbb_neg > 0 else 0\n",
    "        print(f'{model}: {correct}/{total_bbb_neg} ({percentage:.1f}%)')\n",
    "except Exception as e:\n",
    "    print(f'Error in prediction analysis: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aad664",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    \n",
    "    print('\\n' + '='*80)\n",
    "    print('FEATURE IMPORTANCE ANALYSIS')\n",
    "    print('='*80)\n",
    "    \n",
    "    feature_names = X_train.columns\n",
    "    \n",
    "    # Sample data for faster permutation importance computation\n",
    "    X_sample, _, y_sample, _ = train_test_split(\n",
    "        X_train, y_train, train_size=0.3, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f'\\nComputing feature importance using sample data ({X_sample.shape[0]} samples)...')\n",
    "    \n",
    "    # Dictionary to store importance results\n",
    "    importance_results = {}\n",
    "    \n",
    "    # Models to analyze\n",
    "    models_to_analyze = ['RF', 'XGB', 'LR', 'KNN', 'SVM', 'ET', 'LGBM']\n",
    "    \n",
    "    for model_name in models_to_analyze:\n",
    "        try:\n",
    "            if model_name not in padel_saved_models:\n",
    "                print(f'Skipping {model_name} - not in trained models')\n",
    "                continue\n",
    "                \n",
    "            model = padel_saved_models[model_name]\n",
    "            print(f'\\nAnalyzing {model_name}...')\n",
    "            \n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                # For tree-based models (RF, XGB, ET) - FAST\n",
    "                print(f'  Using built-in feature importance')\n",
    "                importances = model.feature_importances_\n",
    "                \n",
    "            elif hasattr(model, 'coef_'):\n",
    "                # For linear models (LR) - Use absolute coefficients\n",
    "                print(f'  Using absolute coefficient values')\n",
    "                importances = np.abs(model.coef_[0])\n",
    "                \n",
    "            else:\n",
    "                # For other models (KNN, SVM) - Use permutation importance\n",
    "                print(f'  Computing permutation importance (this may take ~1-2 min)...')\n",
    "                perm_importance = permutation_importance(\n",
    "                    model, X_sample, y_sample, \n",
    "                    n_repeats=5,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                importances = perm_importance.importances_mean\n",
    "            \n",
    "            # Create DataFrame and get top 20 features\n",
    "            ft_importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importances\n",
    "            }).sort_values(by='Importance', ascending=False).head(20)\n",
    "            \n",
    "            importance_results[model_name] = ft_importance_df\n",
    "            \n",
    "            # Plotting\n",
    "            plt.figure(figsize=(12, 7))\n",
    "            sns.barplot(x='Importance', y='Feature', data=ft_importance_df, palette='viridis')\n",
    "            plt.title(f'Top 20 Most Important Features ({model_name})', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Importance Score')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'../figures/padel_{model_name}_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "            \n",
    "            print(f'\\n  Top 5 features for {model_name}:')\n",
    "            print(ft_importance_df.head(5).to_string())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Error analyzing {model_name}: {e}')\n",
    "            continue\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'Error in feature importance analysis: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de843aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (1238202667.py, line 23)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m**a) K-Nearest Neighbors (KNN, k=3)**\u001b[39m\n       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "## PADEL BBB Prediction Model Pipeline - Design Choices\n",
    "\n",
    "### Overview\n",
    "This pipeline builds classification models to predict Blood-Brain Barrier (BBB) permeability using PADEL molecular descriptors.\n",
    "\n",
    "### Design Choices and Rationale\n",
    "\n",
    "#### 1. **Data Preparation & Preprocessing**\n",
    "- **Descriptor Selection**: Using PADEL descriptors (1876 features) starting from 'nAcid' onwards, excluding molecule identifiers (name, SMILES)\n",
    "- **Missing Value Handling**: Simple zero-filling for missing descriptor values - appropriate for descriptor matrices\n",
    "- **Feature Standardization**: StandardScaler applied to all features\n",
    "  - **Why**: Essential for distance-based algorithms (KNN, SVM) and regularized models (LR)\n",
    "  - Molecular descriptors have different scales and ranges\n",
    "\n",
    "#### 2. **Class Balancing with SMOTE**\n",
    "- **Technique**: Synthetic Minority Over-sampling (SMOTE)\n",
    "- **Why**: BBB prediction typically has class imbalance (more negative than positive permeability)\n",
    "- **Benefits**: Prevents models from biasing toward the majority class\n",
    "\n",
    "#### 3. **Model Selection**\n",
    "Chosen models based on LazyPredict screening and known effectiveness for molecular properties:\n",
    "\n",
    "**a) K-Nearest Neighbors (KNN, k=3)**\n",
    "- **Pros**: Non-linear, fast, good for local patterns\n",
    "- **Cons**: Sensitive to feature scaling (handled by standardization)\n",
    "- **Use case**: Baseline for non-parametric approach\n",
    "\n",
    "**b) Support Vector Machine (SVM)**\n",
    "- **Pros**: Excellent for high-dimensional data (1876 features)\n",
    "- **Theoretical strength**: Natural fit for molecular descriptor spaces\n",
    "- **Cons**: Slower for very large datasets\n",
    "- **Key feature**: Probability estimates via Platt scaling\n",
    "\n",
    "**c) Random Forest (RF, max_depth=8, n_estimators=100)**\n",
    "- **Pros**: Handles feature interactions, feature importance, robust\n",
    "- **Max_depth=8**: Prevents overfitting on high-dimensional data\n",
    "- **Use case**: Feature importance analysis, interpretability\n",
    "\n",
    "**d) Logistic Regression (LR, max_iter=1000)**\n",
    "- **Pros**: Fast, interpretable, provides probability calibration\n",
    "- **Use case**: Linear baseline for comparison\n",
    "- **max_iter=1000**: Ensures convergence on large feature space\n",
    "\n",
    "**e) XGBoost**\n",
    "- **Pros**: State-of-the-art gradient boosting, handles feature interactions\n",
    "- **Separate training**: Uses validation split (not cross-validation) due to sklearn incompatibility\n",
    "- **Use case**: Often achieves best predictive performance\n",
    "\n",
    "**f) Extra Trees (ET, n_estimators=100)**\n",
    "- **Pros**: Robust, handles feature interactions, provides feature importance\n",
    "- **Use case**: Similar to RF but with more randomness\n",
    "\n",
    "**g) LightGBM (LGBM)**\n",
    "- **Pros**: Fast, efficient, handles large datasets well\n",
    "- **Use case**: Gradient boosting alternative to XGBoost\n",
    "\n",
    "#### 4. **Evaluation Strategy**\n",
    "\n",
    "**Cross-Validation Approach**:\n",
    "- **Method**: 5-Fold Stratified K-Fold for training set\n",
    "- **Why**: Ensures balanced class distribution in each fold\n",
    "- **Metrics tracked**:\n",
    "  - **Accuracy**: Overall correctness\n",
    "  - **Precision**: Reliability of positive predictions (important for drug design)\n",
    "  - **Recall (Sensitivity)**: Ability to identify BBB+ compounds\n",
    "  - **Specificity**: Ability to identify BBB- compounds\n",
    "  - **F1 Score**: Balanced metric for imbalanced classes\n",
    "  - **ROC-AUC**: Model discrimination ability\n",
    "  - **PR-AUC**: Precision-Recall curve (more informative for imbalance)\n",
    "  - **MCC**: Matthews Correlation Coefficient (robust for imbalance)\n",
    "\n",
    "**Test Set Evaluation**:\n",
    "- Independent 20% test set for unbiased performance assessment\n",
    "- Class-specific accuracy analysis (% correct BBB+ and BBB- predictions)\n",
    "\n",
    "#### 5. **Feature Importance Analysis**\n",
    "Different approaches for different model types:\n",
    "- **Tree-based (RF, XGB, ET)**: Built-in `feature_importances_` (fast, based on Gini/gain)\n",
    "- **Linear (LR)**: Absolute coefficient magnitudes (interpretable weights)\n",
    "- **Distance-based (KNN, SVM)**: Permutation importance (slower but model-agnostic)\n",
    "- **Top 20 features**: Extracted and visualized for interpretability\n",
    "\n",
    "### Key Implementation Details\n",
    "\n",
    "1. **Error Handling**: Try-except blocks with printed error messages for debugging\n",
    "2. **Reproducibility**: Fixed random_state=42 throughout\n",
    "3. **Parallelization**: n_jobs=-1 for multi-core processing\n",
    "4. **Hyperparameters**: Conservative defaults; could be optimized with Optuna (as in mtoralzml.ipynb)\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "1. **High-dimensional data handling**: Models should handle 1876 descriptor features effectively\n",
    "2. **Class balance impact**: SMOTE should improve recall for minority BBB+ class\n",
    "3. **Model comparison**: RF and XGB typically outperform simpler models\n",
    "4. **Feature insights**: Identify key descriptor patterns for BBB permeability\n",
    "\n",
    "### Next Steps (Optional Enhancements)\n",
    "\n",
    "1. **Hyperparameter Tuning**: Use Optuna (similar to mtoralzml.ipynb) for models with best cross-val performance\n",
    "2. **Ensemble Methods**: Combine predictions from multiple models\n",
    "3. **Feature Selection**: Reduce dimensionality using top important features\n",
    "4. **Domain Knowledge**: Validate identified important descriptors against literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce0da8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09e555b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cede472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dccbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70449eac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58962c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ddf3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c256bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85041f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966c543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd2056",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8453c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8f139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc82098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd5491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d80a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e96e1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333013a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44909d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
